% update for ECCV'14 by Michael Stark and Mario Fritz
% updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
%\usepackage{ruler}
\usepackage{array}
\usepackage{float}
\usepackage{subfig,caption}
\usepackage{lipsum}
\usepackage{comment}
\renewcommand{\arraystretch}{1.1}

\newcommand{\todo}[1]{{\color{red} {\bf TODO:} \it #1}}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\title{Analyzing the Performance of Multilayer Neural Networks for Object Recognition} % Replace with your title

\titlerunning{Analyzing The Performance of Multilayer Neural Networks}


\authorrunning{Agrawal, Girshick, Malik}

\author{Pulkit Agrawal, Ross Girshick, Jitendra Malik\\
\texttt{\{pulkitag,rbg,malik\}@eecs.berkeley.edu}}
\institute{University of California Berkeley}


\maketitle

\begin{abstract}
In 2012 Krizhevsky et al. demonstrated that a Convolutional Neural Network (CNN) trained on large amount of data as part of the ImageNet challenge significantly outperformed computer vision approaches based on hand engineered features. Subsequently, Donahue et al. showed the general usefulness of CNN features on several classification datasets (DeCAF). Concurrently, Girshick et al. used CNN features computed on bottom-up region proposals to dramatically outperform existing object detection methods on PASCAL VOC and ImageNet detection (R-CNN). These results suggest that computer vision is undergoing a feature revolution akin to the one following SIFT and HOG nearly a decade ago. It is therefore important to gain insight into the features learned by these networks and better understand the behaviour of their training algorithms. Towards this direction, our paper is an empirical study addressing the following four questions: 
(1) What happens during fine-tuning of a discriminatively pretrained network? 
(2) How important is a feature's spatial location and its activation magnitude?
(3) Does a multilayer CNN contain ``grandmother'' cells?
(4) How does task performance vary as a function of CNN training time?

\keywords{convolutional neural networks, object recognition, empirical analysis}
\end{abstract}

\input{introduction.tex}
\input{methods.tex}
\input{loc_bin.tex}
\input{grandmother.tex}
\input{finetune.tex}
\input{training_progress.tex}


\section{Conclusion}
\label{sec:conclusion}
In this paper we analysed different properties of convolutional neural networks with the aim of gaining insights required to efficiently exploit the rich feature hierarchies provided by such networks. \\

\noindent \textbf{Acknowledgements}
We thank NVIDIA for donating GPUs. We thank Bharath Hariharan, Saurabh Gupta and Joao Carreira for the discussions and helpful suggestions. Pulkit Agrawal is supported on Fulbright Science and Technology fellowship.  

\begin{align}
  \psi (u) & = \int_{0}^{T} \left[\frac{1}{2}
  \left(\Lambda_{0}^{-1} u,u\right) + N^{\ast} (-u)\right] dt \; \\
& = 0 ?
\end{align}


\bibliographystyle{splncs03}
\bibliography{egbib}
\end{document}
