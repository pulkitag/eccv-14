% update for ECCV'14 by Michael Stark and Mario Fritz
% updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
%\usepackage{ruler}
\usepackage{array}
\usepackage{float}
\usepackage{subfig,caption}
\usepackage{lipsum}
\usepackage{comment}
\renewcommand{\arraystretch}{1.1}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\title{Analyzing the Performance of Multilayer Neural Networks for Object Recognition} % Replace with your title

\titlerunning{Analyzing The Performance of Multilayer Neural Networks}


\authorrunning{Pulkit Agrawal, Ross Girshick, Jitendra Malik}

\author{Pulkit Agrawal, Ross Girshick, Jitendra Malik\\
\texttt{\{pulkitag,rbg,malik\}@eecs.berkeley.edu}}
\institute{University of California Berkeley}


\maketitle

\begin{abstract}
Over the last two years, convolutional neural networks (CNN) have rapidly outperformed methods based on engineered features on a wide variety of visual recognition tasks. This trend started in 2012 with Krizhevsky et al.'s breakthrough results in the ImageNet classification challenge. Subsequently, Donahue et al. demonstrated the general usefulness of CNN features on several classification datasets (DeCAF). Concurrently, Girshick et al. used CNN features computed on bottom-up region proposals to dramatically outperform existing object detection methods on PASCAL VOC and ImageNet detection (R-CNN). These results suggest that computer vision is undergoing a feature revolution akin to the one following SIFT and HOG nearly a decade ago. It is therefore important to gain insight into the features learned by these networks and better understand the behaviour of their training algorithms. Towards this direction, our paper is an empirical study of addressing four questions: 
(1) What happens during fine-tuning of a discriminatively pretrained network? 
(2) How important is a feature's spatial location and its activation magnitude?
(3) Does a multilayer CNN contain ``grandmother'' cells?
(4) How does task performance vary as a function of CNN training time?

\keywords{convolutional neural networks, object recognition, empirical analysis}
\end{abstract}

\input{introduction.tex}
\input{methods.tex}
\input{loc_bin.tex}
\input{grandmother.tex}
\input{finetune.tex}
\input{training_progress.tex}


\section{Conclusion}
\label{sec:conclusion}
In this paper we analysed different properties of convolutional neural networks with the aim of gaining insights required to efficiently exploit the rich feature hierarchies provided by such networks. \\

\noindent \textbf{Acknowledgements}

\begin{align}
  \psi (u) & = \int_{0}^{T} \left[\frac{1}{2}
  \left(\Lambda_{0}^{-1} u,u\right) + N^{\ast} (-u)\right] dt \; \\
& = 0 ?
\end{align}


\bibliographystyle{splncs03}
\bibliography{egbib}
\end{document}
