\relax 
\citation{AmitGeman}
\citation{Breiman}
\@writefile{toc}{\contentsline {title}{Analyzing The Performance of Multilayer Neural Networks for Object Recognition: Supplementary Material}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Anonymous ECCV submission}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}On Using Entropy For Measuring Change in Selectivity}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Method of Entropy Computation}{1}}
\newlabel{sub:def-label-ent}{{1.2}{1}}
\@writefile{toc}{\contentsline {subsubsection}{Label Entropy}{1}}
\citation{Pascal}
\newlabel{sub:def-weighted-label-ent}{{1.2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Weighted Label Entropy}{2}}
\newlabel{sub:def-spmax-label-ent}{{1.2}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Spatial-Max (spMax) Label Entropy}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Mean Cumulative Area Under Entropy Curve (MCAuE Index)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Discussion}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces This table lists percentage decrease in MCAuE as a result of finetuning when only 0.1, 0.25, 0.50 and 1.00 fraction of all the filters were used for computing MCAUE. A negative value indicated increase in entropy. Note that for all the metrics maximum decrease in entropy takes place while moving from layer 5 to layer 7.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:fine-change}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean Cumulative AuE plotted as fraction of filters for all layers of the Conv-Net. Dash-Dot Line: Alex-Net, Solid Line: Fine-Tuned Network. The top plot shows entropy calculated using Weighted Label-Entropy Method, whereas the bottom plot entropy calculated using spMax Label-Entropy Method. (see sec 1.2\hbox {} for method definitions.)\relax }}{3}}
\newlabel{fig:fine-entropy}{{1}{3}}
\citation{Breiman}
\citation{AmitGeman}
\citation{Barlow}
\citation{Grandmother}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Precision-Recall Curves for 20 PASCAL Classes calculated using pool-5 filter responses on ground truth bounding boxes. Red-Curves: Fine Tuned Network, Blue-Curves: Alex-Net. The figure only displays 5 best filters for each class based on AP upto a recall threshold of 0.25. For most classes, precision drops significantly even at modest recall values.\relax }}{4}}
\newlabel{fig:ap}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}On Grand-Mother Cells}{4}}
\citation{Rcnn}
\citation{Rcnn}
\bibstyle{splncs}
\bibdata{egbib}
\bibcite{AmitGeman}{1}
\bibcite{Breiman}{2}
\bibcite{Pascal}{3}
\bibcite{Barlow}{4}
\bibcite{Grandmother}{5}
\bibcite{Rcnn}{6}
\@writefile{toc}{\contentsline {section}{\numberline {3}Epoch Numbers}{5}}
