\relax 
\citation{R}
\@writefile{toc}{\contentsline {title}{Author Guidelines for ECCV Submission}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Anonymous ECCV submission}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{alex}
\citation{caffe}
\citation{rcnn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Relevant Work}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Network-Architecture}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Training Conv-Nets}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fine-Tuning}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning for PASCAL}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discriminative Pre-Training + Fine-Tuning Helps}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Effect of fine-tuning a network for 3 tasks in the PASCAL VOC-2007 Challenge\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:fine-effect}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Layer-Wise Effect of Fine-Tuning}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Layerwise effect of fine-tuning, GT-bbox classification, FT: Fine-Tuned, C-Net: Caffe-Net\relax }}{3}}
\newlabel{table:headings}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}FC-Only Fine-Tuning is Sufficient}{3}}
\citation{rcnn}
\citation{rcnn}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Detection: Fine-Tuning Effects.\relax }}{4}}
\newlabel{table:det-fine}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Replacing fc-layers by other-non linear classifiers}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Speed of Learning}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Evolution of Layer 1 Filters.\relax }}{5}}
\newlabel{fig:conv1}{{1}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Variation in classification accuracy (mean-AP) on PASCAL VOC 2007 challenge using features extracted from different layers of Alex-Net as a function of number of iterations.\relax }}{6}}
\newlabel{table:headings}{{4}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces What happens when we start from a network tuned only for a small number of iterations.\relax }}{6}}
\newlabel{table:det-trajectory}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}What are different layers encoding ?}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of different features used in analysis described in sec. Each 3*3 block in first column is a feature map, the second column is obtained after applying an independent random shuffle to each feature map, third column (spatial max) takes the max activation value in each featue map, fourth shows the feature vector after binarization whereas the last column selects the maximum value in each feature map and also retains its location. \relax }}{7}}
\newlabel{fig:features}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}How Distributed is the Code ?}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}How important is the magnitude of activation ?}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}How important is where a certain filter activates ?}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Detection: Effect of various feature transformations.\relax }}{8}}
\newlabel{table:headings}{{6}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Mean AP on PASCAL-VOC 2007 Classification\relax }}{8}}
\newlabel{table:headings}{{7}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Other Stuff}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Comparing classification performance on ground truth boxes of a network trained on imagenet to one fine-tuned for pascal.\relax }}{9}}
\newlabel{table:headings}{{8}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {7}blah}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}How good are individual layers?}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Distributed Code v/s Tuned Units}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Mean AP on PASCAL-VOC 2007 Classification\relax }}{10}}
\newlabel{table:headings}{{9}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The plot shows the variation in entropy of different layers of a convolutional network trained on imagenet (dot-dash line) and a network fine-tuned for object detection on PASCAL dataset.\relax }}{10}}
\newlabel{fig:example}{{4}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Entropy filters at various layers.\relax }}{11}}
\newlabel{fig:example}{{3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Number of useful dimensions based on the spatial max criterion. (GT-BBOX Classification - Fine Tuned N/W)\relax }}{11}}
\newlabel{fig:example}{{5}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions and Open-Challenges}{12}}
\bibstyle{splncs}
\bibdata{egbib}
