\relax 
\citation{R}
\@writefile{toc}{\contentsline {title}{Author Guidelines for ECCV Submission}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Anonymous ECCV submission}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{alex}
\citation{caffe}
\citation{rcnn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Network-Architecture}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training Conv-Nets}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Fine-Tuning}{2}}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning for PASCAL}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}How does fine-tuning effect the network ?}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Discriminative Fine Tuning Helps}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}How does the entropy of filters vary in the network ?}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Entropy filters at various layers.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The plot shows the variation in entropy of different layers of a convolutional network trained on imagenet (dot-dash line) and a network fine-tuned for object detection on PASCAL dataset.\relax }}{5}}
\newlabel{fig:example}{{2}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Effect of fine-tuning a network for 3 tasks in the PASCAL VOC-2007 Challenge\relax }}{5}}
\newlabel{table:fine-effect}{{1}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Layerwise effect of fine-tuning, GT-bbox classification, FT: Fine-Tuned, C-Net: Caffe-Net\relax }}{5}}
\newlabel{table:headings}{{2}{5}}
\citation{rcnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}FC-Only Fine-Tuning is Sufficient}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Detection: Fine-Tuning Effects.\relax }}{6}}
\newlabel{table:det-fine}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Replacing fc-layers by other-non linear classifiers}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Is}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of different features used in analysis described in sec. Each 3*3 block in first column is a feature map, the second column is obtained after applying an independent random shuffle to each feature map, third column (spatial max) takes the max activation value in each featue map, fourth shows the feature vector after binarization whereas the last column selects the maximum value in each feature map and also retains its location. \relax }}{7}}
\newlabel{fig:features}{{3}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Detection: Effect of various feature transformations.\relax }}{7}}
\newlabel{table:headings}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}How important is the magnitude of activation ?}{7}}
\newlabel{sub:imp-mag}{{4.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}How important is where a certain filter activates ?}{7}}
\newlabel{sub:imp-loc}{{4.2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}How common are Grand-Mother like Units ?}{7}}
\citation{zeiler}
\citation{simoyan}
\citation{google}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Mean AP on PASCAL-VOC 2007 Classification\relax }}{8}}
\newlabel{table:headings}{{5}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Finding class specific units}{8}}
\newlabel{sub:class-specific-unit}{{5.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}How many units do we need?}{9}}
\newlabel{sub:how-many}{{5.2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of our greedy strategy for constructing subsets of filters. For each class we first train a linear-svm using the spatial-max feature transformation described in section 4.2\hbox {}. Spatial-max leaves us with a 256-D vector wherein each dimension has a one to one correspondence with 256 pool-5 filters. We use the magnitude of each dimension of the learnt weight vector as a proxy for the importance of that dimension towards discriminating a given class. For the purpose of illustration we describe the procedure with a 4-D weight vector shown on the extreme left. Firstly, we take the absolute value for each dimension and then sort the dimensions based on this value. Then, we chose the top k filters/dimensions from this ranked list to construct a subset of size k.\relax }}{9}}
\newlabel{fig:sel-strategy}{{4}{9}}
\citation{rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Analysis of how many filters are required to classify ground truth bounding boxes for 20 categories taken from PASCAL-2007 detection challenge. The y-axis in each of plot represents classification accuracy measured as mean-ap where as x-axis stand for the number of filters.)\relax }}{10}}
\newlabel{fig:svm-sel-dims}{{5}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Number of filters required to achieve 50\% ,90\% of the full performance for PASCAL classes using Alex-Net(AN) and the Fine-Tuned network(FT)\relax }}{10}}
\newlabel{table:num-fil}{{6}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Speed of Learning}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Evolution of Layer 1 Filters.\relax }}{11}}
\newlabel{fig:conv1}{{6}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Variation in classification accuracy (mean-AP) on PASCAL VOC 2007 challenge using features extracted from different layers of Alex-Net as a function of number of iterations.\relax }}{12}}
\newlabel{table:headings}{{7}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces What happens when we start from a network tuned only for a small number of iterations.\relax }}{12}}
\newlabel{table:det-trajectory}{{8}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Open-Challenges}{12}}
\bibstyle{splncs}
\bibdata{egbib}
