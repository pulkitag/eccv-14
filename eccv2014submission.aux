\relax 
\citation{Kriz}
\citation{imagenet}
\citation{Decaf}
\citation{Rcnn}
\citation{regionlets}
\citation{Hog}
\citation{Sift}
\citation{Lecun89}
\@writefile{toc}{\contentsline {title}{Analyzing The Performance of Multilayer Neural Networks for Object Recognition}{1}}
\@writefile{toc}{\authcount {1}}
\@writefile{toc}{\contentsline {author}{Anonymous ECCV submission}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{GoogleCat}
\citation{DeepPre}
\citation{HintonPre}
\citation{Decaf}
\citation{Rcnn}
\citation{Grandmother}
\citation{GoogleCat}
\citation{Kriz}
\citation{caffe}
\citation{Pascal}
\citation{Pascal}
\citation{Rcnn}
\citation{sun}
\citation{sun}
\citation{Rcnn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Training Procedure}{3}}
\newlabel{sec:train}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Network-Architecture and Nomenclature}{3}}
\newlabel{sub:net-arch}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training Setup}{3}}
\newlabel{sub:train-setup}{{2.2}{3}}
\newlabel{sub:fine-train}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning}{3}}
\citation{Rcnn}
\citation{Decaf}
\citation{HintonPre}
\citation{DeepPre}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Method of Entropy Computation}{4}}
\newlabel{sub:def-ent}{{2.3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}What happens when a discriminatively pretrained network is finetuned?}{4}}
\newlabel{sec:fine}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Distribution of AuE for different layers in Alex-Net and FT-Net. X-axis is the entropy and the Y-axis is the number of filters. Notice that the left tail for relu 6 and 7 becomes heavier after finetuning. This indicates that finetuning makes these filters more discriminative.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fine-hist}{{1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Entropy Analysis}{5}}
\newlabel{sub:fine-entropy}{{3.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Mean Cumulative AuE plotted as fraction of filters (see sec 3.1\hbox {}). (Dash-Dot Line :Alex-Net, Solid Line: Fine-Tuned Network).\relax }}{6}}
\newlabel{fig:fine-entropy}{{2}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces mAP for gt-bbox classification(FT: Fine-Tuned,A-Net: Alex-Net).\relax }}{6}}
\newlabel{table:gt-bbox-fine}{{1}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison in performance on of Alex-Net, Finetuned Network(ft-net) and a network with only fc layers finetuned (fc-ft).\relax }}{6}}
\newlabel{table:fine-effect}{{2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Is finetuning only the fully-connected layers sufficient?}{6}}
\newlabel{sub:fine-fc-only}{{3.2}{6}}
\citation{Blocks}
\citation{Mid1}
\citation{overfeat}
\citation{Decaf}
\citation{Rcnn}
\citation{DPM}
\citation{YangRamanan}
\citation{Poselets}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Evaluation of effect finetuning towards the task of object detection. (l5, l6, l7: layers 5, 6 and 7 of Alex Net)\relax }}{7}}
\newlabel{table:det-fine}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Discussion}{7}}
\newlabel{sub:fine-discussion}{{3.3}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Is the information in the location or in the magnitude of filter activation?}{7}}
\newlabel{sec-where-info}{{4}{7}}
\citation{Rcnn}
\citation{Rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Description of feature ablations: For the purpose of illustration, consider each 3*3 block in (a) as the spatial activation of individual filters. Each block in (a) is a seperate filter from the same layer. Ablation 1: \textit  {spatial-shuffle-(b)} (sp-shuffle), involves applying an independent spatial random permutation to each feature map (different images see different permuations). Ablation 2: \textit  {spatial max}-(c) (sp-max), select the max activation value in each featue map. Ablation 3: \textit  {binarization}-(d) (bin). Ablation 4: \textit  {sp-max-bin} - binarized sp-max.\relax }}{8}}
\newlabel{fig:features}{{3}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}How important is where a filter activates?}{8}}
\newlabel{sub:imp-loc}{{4.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}How important is the magnitude of activation ?}{8}}
\newlabel{sub:imp-mag}{{4.2}{8}}
\citation{DeConv}
\citation{Simonyan}
\citation{DeConv}
\citation{Simonyan}
\citation{GoogleCat}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Feature ablation analysis on PASCAL Image Classification (see fig 3\hbox {})\relax }}{9}}
\newlabel{table:class-ablation}{{4}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Effect of various feature ablations on object detection using R-CNN\cite  {Rcnn}.\relax }}{9}}
\newlabel{table:det-ablation}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Discussion}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Are there Grand-Mother Cells in CNN's? How distributed is the code?}{9}}
\newlabel{sec:grand-mother}{{5}{9}}
\citation{Barlow}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Are there high precision filters in layer 5?}{10}}
\newlabel{sub:class-specific-unit}{{5.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}How many filters are required for discrimination?}{10}}
\newlabel{sub:how-many}{{5.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces This plot shows the precision curve for the top 5 most selective filters taken from Alex-Net (Blue) and FT-Net(Red) for all PASCAL classes. Y-axis is the precision and X-axis is number of examples.\relax }}{11}}
\newlabel{fig:prob-sel}{{4}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Illustration of our greedy strategy for constructing subsets of filters. For each class we first train a linear-svm using the spatial-max feature transformation described in section 4.1\hbox {}. Spatial-max leaves us with a 256-D vector wherein each dimension has a one to one correspondence with 256 pool-5 filters. We use the magnitude of each dimension of the learnt weight vector as a proxy for the importance of that dimension towards discriminating a given class. For the purpose of illustration we describe the procedure with a 4-D weight vector shown on the extreme left (the numbers on each bar are the "dimension"). Firstly, we take the absolute value for each dimension and then sort the dimensions based on this value. Then, we chose the top k filters/dimensions from this ranked list to construct a subset of size k.\relax }}{11}}
\newlabel{fig:sel-strategy}{{5}{11}}
\citation{Rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Analysis of how many filters are required to classify ground truth bounding boxes for 20 categories taken from PASCAL-2007 detection challenge. The y-axis in each of plot represents classification accuracy measured as mean-ap where as x-axis stand for the number of filters.)\relax }}{12}}
\newlabel{fig:svm-sel-dims}{{6}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Number of filters required to achieve 50\% ,90\% of the full performance for PASCAL classes using Alex-Net(AN) and the Fine-Tuned network(FT)\relax }}{12}}
\newlabel{table:num-fil}{{6}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Discussion}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The first row shows conv-1 filters at 5K, 15K and 225K training iteration. The second row shows the evolution of training loss and top-1 accuracy on imagenet ilsvrc-2012 validation set as a function of number of iterations.\relax }}{13}}
\newlabel{fig:conv1}{{7}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}How do the different layers of a CNN train over time?}{13}}
\newlabel{sec:speed}{{6}{13}}
\bibstyle{splncs}
\bibdata{egbib}
\bibcite{Kriz}{1}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Variation in classification accuracy (mean-AP) on PASCAL VOC 2007 challenge using features extracted from different layers of Alex-Net as a function of number of iterations.\relax }}{14}}
\newlabel{table:det-traj-classify}{{7}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Performance of 50-50 network for detection on pascal-voc-2007 challenge. (l5 is pool-5 and l7 is relu-7)\relax }}{14}}
\newlabel{table:det-trajectory}{{8}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{14}}
\newlabel{sec:conclusion}{{7}{14}}
\bibcite{imagenet}{2}
\bibcite{Decaf}{3}
\bibcite{Rcnn}{4}
\bibcite{regionlets}{5}
\bibcite{Hog}{6}
\bibcite{Sift}{7}
\bibcite{Lecun89}{8}
\bibcite{GoogleCat}{9}
\bibcite{DeepPre}{10}
\bibcite{HintonPre}{11}
\bibcite{Grandmother}{12}
\bibcite{caffe}{13}
\bibcite{Pascal}{14}
\bibcite{sun}{15}
\bibcite{Blocks}{16}
\bibcite{Mid1}{17}
\bibcite{overfeat}{18}
\bibcite{DPM}{19}
\bibcite{YangRamanan}{20}
\bibcite{Poselets}{21}
\bibcite{DeConv}{22}
\bibcite{Simonyan}{23}
\bibcite{Barlow}{24}
