\section{Introduction}
% Over the past two years we've seen a string of work
% Krizhevsky, Donahue, Girshick, and now many more demonstrating that
% convolutional neural networks are able to learn features that
% outperform traditional engineered features in a wide array of tasks.

Over the last two years, a sequence of results on benchmark visual recognition tasks has demonstrated that convolutional neural networks (CNNs) \cite{fukushima1980neocognitron,Lecun89,rumelhart86} will likely replace engineered features, such as SIFT \cite{Sift} and HOG \cite{Hog}, for a wide variety of problems.
This sequence started with the breakthrough ImageNet \cite{imagenet} classification results reported by Krizhevsky et al. \cite{Kriz}.
Soon after, Donahue et al. \cite{Decaf} showed that the same network, trained for ImageNet classification, was an effective blackbox feature extractor.
Using CNN features, they reported state-of-the-art results on several standard image classification datasets.
At the same time, Girshick et al. \cite{Rcnn} showed how the network could be applied to object detection.
Their system, called R-CNN, classifies object proposals generated by a bottom-up grouping mechanism (e.g., selective search \cite{UijlingsIJCV2013}).
Since detection training data is limited, they proposed a transfer learning strategy in which the CNN is first pre-trained, with supervision, for ImageNet classification and then fine-tuned on the small PASCAL detection dataset \cite{Pascal}.
Since this initial set of results, several other papers have reported similar findings on a wider range of tasks (see, for example, the outcomes reported by Razavian et al. in \cite{astounding}).

Feature transforms such as SIFT and HOG afford an intuitive interpretation as histograms of oriented edge filter responses arranged in spatial blocks.
However, we have little understanding of what visual features the different layers of a CNN encode.
Given that rich feature hierarchies provided by CNNs are likely to emerge as the prominent feature extractor for computer vision models over the next few years, we believe that developing such an understanding is an interesting scientific pursuit and an essential exercise that will help guide the design of computer vision methods that use CNNs.
Therefore, in this paper we study several aspects of CNNs through an empirical lens.

% Therefore in this paper, we study several aspects of CNNs
% - What is the nature of the feature representation? (grandmother vs. distributed)
% - How does fine-tuning affect performance?
% - How does pre-training time affect generalization?
% - Untangling feature magnitude and location

\subsection{Summary of findings}

\subsubsection{Effects of fine-tuning and pre-training.} 
Girshick et al. \cite{Rcnn} showed that supervised pre-training and fine-tuning are effective when training data is scarce.
However, they did not investigate what happens when training data becomes more abundant.
We show that it is possible to get good performance when training R-CNN from a random initialization (i.e., without ImageNet supervised pre-training) with a reasonably modest amount of detection training data (37k ground truth bounding boxes).
However, we also show that in this data regime, supervised pre-training is still beneficial and leads to a large improvement in detection performance.
We show similar results for image classification, as well.

\subsubsection{ImageNet pre-training does not overfit.}
One concern when using supervised pre-training is that achieving a better model fit to ImageNet, for example, might lead to higher generalization error when applying the learned features to another dataset and task.
If this is the case, then some form of regularization during pre-training, such as early stopping, would be beneficial.
We show the surprising result that pre-training for longer yields better results, with diminishing returns, but does \emph{not} increase generalization error.
This implies that fitting the CNN to ImageNet induces a general and portable feature representation.
Moreover, the learning process is well behaved and does not require ad hoc regularization in the form of early stopping.

\subsubsection{Grandmother cells and distributed codes.}
%Recent work on visualizing features from deep networks (e.g., \cite{GoogleCat,DeConv}) suggests that the features might consist mainly of ``grandmother'' cells \cite{Barlow,Grandmother}.
%In other words, that the CNN learns features that fire only when a specific semantic class, such as \emph{cat}, is present.
We do not have a good understanding of mid level feature representations in multi layer networks.
Recent work on visualization features, (e.g., \cite{GoogleCat,DeConv}) suggests that such networks might consist mainly of ``grandmother'' cells \cite{Barlow,Grandmother}.
%Our analysis shows that the representation is more subtle.
Our analysis shows that representation in intermediate layers is more subtle.
There are a small number of grandmother-cell-like features, but most of the feature code is distributed and several features must fire in concert to effectively discriminate between classes.

\subsubsection{Importance of feature location and magnitude.}
Our final set of experiments investigates what role a feature's spatial location and magnitude plays in image classification and object detection.
Matching intuition, we find that spatial location is critical for object detection, but matters little for image classification.
More surprisingly, we find that feature magnitude is largely unimportant.
For example, binarizing features (at a threshold of 0) barely degrades performance.
This shows that sparse binary features, which are useful for large-scale image retrieval \cite{gong2011iterative,weiss2009spectral}, come ``for free'' from the CNN's representation.

%Proponents of multilayer networks have argued in the past that unsupervised pre-training followed by finetuning is helpful for learning features which improve performance on discriminative tasks such as image classification \cite{GoogleCat, DeepPre, HintonPre}. Finetuning a network is the process of slowly updating pre-learned parameters to minimize a target loss function for a new task at hand. Since, multilayer networks consist of a large number of parameters they are prone to overfitting when trained on small datasets. Instead of unsupervised pre-training, \cite{Decaf, Rcnn} have made a strong case for learning features using discriminative pre-training followed by finetuning for a specific task at hand. They first trained a network for the task of image classification on Imagenet and then finetuned the network for object detection on PASCAL. In this work, we analyse the effect of finetuning on different layers of a CNN (section \ref{sec:fine}). We find that for moderately sized datasets (upto $\sim$ 50K images), finetuning only effects the top two fully connected layers (section \ref{sub:net-arch}) whereas the lower convolutional layers are mostly unchanged. We show that this observation can be used to make finetuning 2x faster with negligible effect on performance. We also demonstrate that a network can be trained from scratch using data only from the PASCAL dataset to achieve the state of art performance on object detection reported by \cite{Rcnn}. Further, by simply using more data for finetuning, we report a $10\%$ increase in performance for object detection over \cite{Rcnn}. 

%Finetuning can also be viewed as a method for transfer learning (NEED REF for transfer learning). It is interesting to draw a parallel between this and the way we humans learn. As children, we can easily learn new things but as we grow older it becomes harder. Similarly, it is possible that if CNNs are pretrained for too long, it becomes harder to generalize to a new task. In other words, pre-training can lead to overfitting and consequently lead to worse performance on a new task due to dataset bias \cite{datasetBias}. Thus, we are faced with the question - is there an optimal time for which pre-training should be carried out? Rather surprisingly, we find that fitting better to ImageNet allows lower generalization error when moving to other datasets (section \ref{sec:train}), i.e. pre-training more improves performance.
