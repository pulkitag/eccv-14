\section{Conclusion}
To help researchers better understand CNNs, we investigated pre-training and fine-tuning behavior on three classification and detection datasets.
We found that the large CNN used in this work can be trained from scratch using a surprisingly modest amount of data.
But, importantly, pre-training significantly improves performance and pre-training for longer is better.
We also found that some of the learnt CNN features are grandmother-cell-like, but for the most part, they form a distributed code.
This supports the recent set of empirical results showing that these features generalize well to other datasets and tasks.
